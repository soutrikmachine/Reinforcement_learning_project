{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# DAT257x: Reinforcement Learning Explained\n\n## Lab 2: Bandits\n\n### Exercise 2.1A: Greedy policy"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport sys\n\nif \"../\" not in sys.path:\n    sys.path.append(\"../\") \n\nfrom lib.envs.bandit import BanditEnv\nfrom lib.simulation import Experiment",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's define an interface of a policy. For a start, the policy should know how many actions it can take and able to take a particular action given that policy"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "#Policy interface\nclass Policy:\n    #num_actions: (int) Number of arms [indexed by 0 ... num_actions-1]\n    def __init__(self, num_actions):\n        self.num_actions = num_actions\n    \n    def act(self):\n        pass\n        \n    def feedback(self, action, reward):\n        pass",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now let's implement a greedy policy based on the policy interface. The greedy policy will take the most rewarding action (i.e greedy). This is implemented in the act() function. In addition, we will maintain the name of the policy (name), the rewards it has accumulated for each action (total_rewards), and the number of times an action has been performed (total_counts)."
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "#Greedy policy\nclass Greedy(Policy):\n    def __init__(self, num_actions):\n        Policy.__init__(self, num_actions)\n        self.name = \"Greedy\"\n        self.total_rewards = np.zeros(num_actions, dtype = np.longdouble)\n        self.total_counts = np.zeros(num_actions, dtype = np.longdouble)\n    \n    def act(self):\n        current_averages = np.divide(self.total_rewards, self.total_counts, where = self.total_counts > 0)\n        current_averages[self.total_counts <= 0] = 0.5      #Correctly handles Bernoulli rewards; over-estimates otherwise\n        current_action = np.argmax(current_averages)\n        return current_action\n        \n    def feedback(self, action, reward):\n        self.total_rewards[action] += reward\n        self.total_counts[action] += 1",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We are now ready to perform our first simulation. Let's set some parameters."
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "evaluation_seed = 8026\nnum_actions = 5\ntrials = 10000\ndistribution = \"bernoulli\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now, put the pieces together and run the experiment."
    },
    {
      "metadata": {
        "collapsed": true,
        "scrolled": false,
        "trusted": false
      },
      "cell_type": "code",
      "source": "env = BanditEnv(num_actions, distribution, evaluation_seed)\nagent = Greedy(num_actions)\nexperiment = Experiment(env, agent)\nexperiment.run_bandit(trials)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "Observe the above results and answer the lab questions!"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "file_extension": ".py",
      "version": "3.5.4",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}